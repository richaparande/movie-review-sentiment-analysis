{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Enjoy the opening credits. They're the best th...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Well, the Sci-Fi channel keeps churning these ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>It takes guts to make a movie on Gandhi in Ind...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Nest is really just another 'nature run am...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Waco: Rules of Engagement does a very good job...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text sentiment\n",
       "0           0  Enjoy the opening credits. They're the best th...       neg\n",
       "1           1  Well, the Sci-Fi channel keeps churning these ...       neg\n",
       "2           2  It takes guts to make a movie on Gandhi in Ind...       pos\n",
       "3           3  The Nest is really just another 'nature run am...       neg\n",
       "4           4  Waco: Rules of Engagement does a very good job...       pos"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data and take a quick look\n",
    "import pandas as pd\n",
    "raw_data = pd.read_csv('coursework1_train.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of reviews: 40000\n",
      "number of pos reviews: 20000\n",
      "number of neg reviews: 20000\n"
     ]
    }
   ],
   "source": [
    "# check the size of the data and its class distribution\n",
    "all_text = raw_data['text'].tolist()\n",
    "all_lables = raw_data['sentiment'].tolist()\n",
    "\n",
    "print('number of reviews:', len(all_text))\n",
    "print('number of pos reviews:', len([l for l in all_lables if l=='pos']))\n",
    "print('number of neg reviews:', len([l for l in all_lables if l=='neg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first piece of preprocessing I performed is converting the text to lowercase, in case I have to remove stopwords later (nltk's stopwords are all in lowercase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text to lowercase\n",
    "raw_data['text'] = raw_data['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this, I lemmatized the text so I can correctly identify the frequency of words later (e.g. movie and movies should not be counted as two separate tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize text\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "raw_data['text'] = raw_data['text'].apply(lambda x: \" \".join([wordnet_lemmatizer.lemmatize(y) for y in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then removed all punctuation, in order to make clean and word-only tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "import re\n",
    "raw_data['text'] = raw_data['text'].apply(lambda x: re.sub('[^\\w\\s]', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step to start analyzing the text is tokenizing it. I performed this in order to find the most frequent words in the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text\n",
    "from nltk import word_tokenize\n",
    "raw_data['review_tokens'] = raw_data['text'].apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>enjoy the opening credits theyre the best thin...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[enjoy, the, opening, credits, theyre, the, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>well the scifi channel keep churning these tur...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[well, the, scifi, channel, keep, churning, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>it take gut to make a movie on gandhi in india...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[it, take, gut, to, make, a, movie, on, gandhi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the nest is really just another nature run amo...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[the, nest, is, really, just, another, nature,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>waco rule of engagement doe a very good job of...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[waco, rule, of, engagement, doe, a, very, goo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text sentiment  \\\n",
       "0           0  enjoy the opening credits theyre the best thin...       neg   \n",
       "1           1  well the scifi channel keep churning these tur...       neg   \n",
       "2           2  it take gut to make a movie on gandhi in india...       pos   \n",
       "3           3  the nest is really just another nature run amo...       neg   \n",
       "4           4  waco rule of engagement doe a very good job of...       pos   \n",
       "\n",
       "                                       review_tokens  \n",
       "0  [enjoy, the, opening, credits, theyre, the, be...  \n",
       "1  [well, the, scifi, channel, keep, churning, th...  \n",
       "2  [it, take, gut, to, make, a, movie, on, gandhi...  \n",
       "3  [the, nest, is, really, just, another, nature,...  \n",
       "4  [waco, rule, of, engagement, doe, a, very, goo...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take another quick look at the data to make sure preprocessing is working\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to take a look at the most frequent words in a sample review to get a bigger picture of the data, and in order to determine whether any further preprocessing is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 34), ('a', 25), ('and', 21), ('of', 19), ('to', 18)]\n"
     ]
    }
   ],
   "source": [
    "# get frequency distribution of words\n",
    "tokens = raw_data['review_tokens'].tolist()\n",
    "fdist = nltk.FreqDist(tokens[10])\n",
    "print(fdist.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like all the top frequent words are stopwords! It's important to remove these words from the text in order to improve the accuracy of tf-idf implemented later, and the machine learning model too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "sw = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return [x for x in text if x not in sw]\n",
    "\n",
    "cleaned_text = raw_data['review_tokens'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'enjoy opening credits theyre best thing secondrate inoffensive timekiller feature passable performance like eric robert martin kove main part however go newcomer tommy lee thomas look bit diminutive kind action nevertheless occasionally manages project bantyrooster kind belligerence first time see hes barechested sweaty engaged favorite beefcake activity chopping wood ha seven scene without shirt including one hes hanged wrist zapped electricity la mel gibson lethal weapon could use better script however since manner expose truth corruption violence inside prison never convincing theres also talk million dollar apparently tied investigation never explained pluses though sending john woodrow undercover john wilson amusing play presidential name costar jody ross nolan show promise inmate early proceedings shown hanged wrist getting punched burly guard one final note movies low budget painfully responsible lack extras despite impressive size prison seems hold 12 inmates note cast credit end arent helpful record burly baldheaded guard us jody nolan punching bag played bill fishback young fairhaired guard administers electric shock tommy lee thomas played marc chenail'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recreate the reviews in the correct format for developing the model\n",
    "final_text = [' '.join(x) for x in cleaned_text]\n",
    "final_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned review looks ideal for analysis, with lemmatization done and stopwords removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to split the data. I chose an 80/20 split for the train and test sets as this is known to be a fair split in machine learning processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "train_text = final_text[:32000]\n",
    "train_labels = all_lables[:32000]\n",
    "test_text = final_text[32000:]\n",
    "test_labels = all_lables[32000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing and training the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf is an ideal method to use for feature extraction in this text analysis as it would outline the key words required to correctly categorize the data.\n",
    "I had initially decided not to set a max feature number, as limiting the features produced results with a lower accuracy. However, I realized that this led to overfitting of the data, so I used a max feature number of 2000.\n",
    "\n",
    "I believe logistic regression is a good algorithm to use in this case because the labels are binary - pos and neg. Since logistic regression is one of the most common methods for binary classification, this is a safe bet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.86925\n",
      "precision: 0.8692932727465921\n",
      "rec: 0.8692857942885943\n",
      "f1: 0.8692499264530836\n"
     ]
    }
   ],
   "source": [
    "# tf-idf and logistic regression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "max_feature_num = 2000\n",
    "train_vectorizer = TfidfVectorizer(max_features=max_feature_num)\n",
    "train_vecs = train_vectorizer.fit_transform(train_text)\n",
    "test_vecs = TfidfVectorizer(max_features=max_feature_num,vocabulary=train_vectorizer.vocabulary_).fit_transform(test_text)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_lr = LogisticRegression(solver='lbfgs').fit(train_vecs, train_labels)\n",
    "\n",
    "# make prediction\n",
    "test_pred_lr = clf_lr.predict(test_vecs)\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "acc_lr = accuracy_score(test_labels, test_pred_lr)\n",
    "pre_lr, rec_lr, f1_lr, _ = precision_recall_fscore_support(test_labels, test_pred_lr, average='macro')\n",
    "print('accuracy:', acc_lr)\n",
    "print('precision:', pre_lr)\n",
    "print('rec:', rec_lr)\n",
    "print('f1:', f1_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of 0.87 is good for this data, but I'm also interested in seeing how other algorithms perform, so I will try tf-idf with Naive Bayes next as this is another common algorithm for text classification. I use the same feature limitation in tf-idf so as to standardize performance of the two algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.841625\n",
      "precision: 0.8416383585551622\n",
      "rec: 0.8416470492937747\n",
      "f1: 0.8416247005716995\n"
     ]
    }
   ],
   "source": [
    "# tf-idf and naive bayes\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "max_feature_num = 2000\n",
    "train_vectorizer = TfidfVectorizer(max_features=max_feature_num)\n",
    "train_vecs = train_vectorizer.fit_transform(train_text)\n",
    "test_vecs = TfidfVectorizer(max_features=max_feature_num,vocabulary=train_vectorizer.vocabulary_).fit_transform(test_text)\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_nb = MultinomialNB().fit(train_vecs, train_labels)\n",
    "\n",
    "# make prediction\n",
    "test_pred_nb = clf_nb.predict(test_vecs)\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "acc_nb = accuracy_score(test_labels, test_pred_nb)\n",
    "pre_nb, rec_nb, f1_nb, _ = precision_recall_fscore_support(test_labels, test_pred_nb, average='macro')\n",
    "print('accuracy:', acc_nb)\n",
    "print('precision:', pre_nb)\n",
    "print('rec:', rec_nb)\n",
    "print('f1:', f1_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an accuracy rate of 0.84, Naive Bayes is slightly less effective than logistic regression. Therefore, logistic regression is the better machine learning algorithm for this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save model and other necessary modules\n",
    "info_save = {\n",
    "    'model': clf_lr,\n",
    "    'vectorizer': TfidfVectorizer(vocabulary=train_vectorizer.vocabulary_)\n",
    "}\n",
    "save_path = open(\"trained_model.pickle\",\"wb\")\n",
    "pickle.dump(info_save, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
